{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normative modelling for dMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/braincharts/scripts/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from random import sample\n",
    "\n",
    "from pcntoolkit.normative import estimate, predict, evaluate\n",
    "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
    "from nm_utils import calibration_descriptives, remove_bad_subjects, load_2d\n",
    "\n",
    "ukb_root = '/project_freenas/3022017.02/UKB'\n",
    "sys.path.append(os.path.join(ukb_root,'scripts'))\n",
    "from ukb_utils import get_variables_UKB, lookup_UKB\n",
    "ukb_idp_dir = os.path.join(ukb_root,'phenotypes','current')\n",
    "root_dir = '/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/braincharts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sex\n",
    "field_codes = ['eid','31-0.0']\n",
    "field_names = ['eid', 'sex']\n",
    "df_sex, subs = get_variables_UKB(os.path.join(ukb_idp_dir,'01_basic_demographics.csv'), field_codes, field_names)\n",
    "\n",
    "# load age and site\n",
    "field_codes = ['eid', '21003-2.0', '54-2.0']\n",
    "field_names = ['eid', 'age', 'site']\n",
    "df_age, subs = get_variables_UKB(os.path.join(ukb_idp_dir,'99_miscellaneous.csv'), field_codes, field_names)\n",
    "\n",
    "# load dMRI derived phenotypes\n",
    "field_codes = ['eid', '25059-2.0', '25101-2.0']\n",
    "field_names = ['eid', 'FA_corpus_callosum', 'FA_uncinate_fasciculus']\n",
    "df_dmri, subs = get_variables_UKB(os.path.join(ukb_idp_dir,'31_brain_IDPs.csv'), field_codes, field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmri_data = df_sex.join(df_age).join(df_dmri)\n",
    "dmri_data.dropna(inplace=True)\n",
    "dmri_data.to_csv(os.path.join(root_dir,'data_dmri.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11025., 11027., 11026., 11028.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age.site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw data are stored\n",
    "data_dir = os.path.join(root_dir,'data')\n",
    "\n",
    "# where the analysis takes place\n",
    "out_dir = os.path.join(root_dir,'models','test')\n",
    "\n",
    "# create the output directory if it does not already exist\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>site</th>\n",
       "      <th>age</th>\n",
       "      <th>FA_corpus_callosum</th>\n",
       "      <th>FA_uncinate_fasciculus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.682972</td>\n",
       "      <td>0.491452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000432</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11027.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.756112</td>\n",
       "      <td>0.517835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000853</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.713533</td>\n",
       "      <td>0.472923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000871</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11027.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.703294</td>\n",
       "      <td>0.516493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.692061</td>\n",
       "      <td>0.471372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024836</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11026.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.675107</td>\n",
       "      <td>0.507491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6024902</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.700486</td>\n",
       "      <td>0.439941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6025056</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.702473</td>\n",
       "      <td>0.517079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6025069</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.686748</td>\n",
       "      <td>0.468932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6025141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.717520</td>\n",
       "      <td>0.554079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40521 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex     site   age  FA_corpus_callosum  FA_uncinate_fasciculus\n",
       "eid                                                                    \n",
       "1000050  0.0  11025.0  70.0            0.682972                0.491452\n",
       "1000432  1.0  11027.0  70.0            0.756112                0.517835\n",
       "1000853  0.0  11025.0  73.0            0.713533                0.472923\n",
       "1000871  1.0  11027.0  73.0            0.703294                0.516493\n",
       "1000910  0.0  11025.0  69.0            0.692061                0.471372\n",
       "...      ...      ...   ...                 ...                     ...\n",
       "6024836  1.0  11026.0  74.0            0.675107                0.507491\n",
       "6024902  1.0  11025.0  64.0            0.700486                0.439941\n",
       "6025056  1.0  11025.0  71.0            0.702473                0.517079\n",
       "6025069  1.0  11025.0  72.0            0.686748                0.468932\n",
       "6025141  0.0  11025.0  57.0            0.717520                0.554079\n",
       "\n",
       "[40521 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmri_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23158 subjects in total\n",
      "There are 22658 subjects without manually assigned labels\n",
      "There are 17658 subjects for training\n",
      "There are 5000 subjects for testing\n"
     ]
    }
   ],
   "source": [
    "### Load all the subject IDs that have qc data available\n",
    "pickled_data = '/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/subjects_qc.pkl'\n",
    "with open(pickled_data, 'rb') as f: \n",
    "    subs_qc = pickle.load(f)\n",
    "### Load the subjects IDs that have manual qc labels available \n",
    "pickled_data = '/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/labeledsubs_qc.pkl'\n",
    "with open(pickled_data, 'rb') as f: \n",
    "    labeled_qc = pickle.load(f)\n",
    "### Subtract the labeled subs from the total subs\n",
    "keys = list(labeled_qc.columns.values)\n",
    "i1 = subs_qc.set_index(keys).index\n",
    "i2 = labeled_qc.set_index(keys).index\n",
    "unlabeled_subs = subs_qc[~i1.isin(i2)]\n",
    "print('There are', len(subs_qc), 'subjects in total')\n",
    "print('There are', len(unlabeled_subs), 'subjects without manually assigned labels')\n",
    "### Split the subjects into training and testing\n",
    "### Select 5k subjects randomly for the testing dataset\n",
    "# unlabeled_subs = unlabeled_subs.subs.tolist()\n",
    "# subs_test = sample(unlabeled_subs,5000)\n",
    "# subs_test = pd.DataFrame(subs_test)\n",
    "# subs_test.columns = ['subs']\n",
    "# subs_test.to_pickle('/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/subs_test_5k.pkl')\n",
    "pickled_data2 = '/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/subs_test_5k.pkl'\n",
    "with open(pickled_data2, 'rb') as f: \n",
    "    subs_test = pickle.load(f)\n",
    "### Create the training subs list\n",
    "keys = list(subs_test.columns.values)\n",
    "i1 = unlabeled_subs.set_index(keys).index\n",
    "i2 = subs_test.set_index(keys).index\n",
    "subs_train = unlabeled_subs[~i1.isin(i2)]\n",
    "print('There are', len(subs_train), 'subjects for training')\n",
    "print('There are', len(subs_test), 'subjects for testing')\n",
    "# subs_train.to_pickle('/home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/subs_train_5k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the training and testig dataset based on the subject lists\n",
    "df_tr = dmri_data[dmri_data.index.isin(subs_train.subs)]\n",
    "df_te = dmri_data[dmri_data.index.isin(subs_test.subs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eid\n",
       "1001815    11025.0\n",
       "1002690    11027.0\n",
       "1002758    11027.0\n",
       "1004244    11027.0\n",
       "1004617    11027.0\n",
       "            ...   \n",
       "6022149    11025.0\n",
       "6023174    11025.0\n",
       "6023696    11025.0\n",
       "6023777    11025.0\n",
       "6025069    11025.0\n",
       "Name: site, Length: 4998, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_ids = df_te['site']\n",
    "site_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure which models to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(root_dir,'data','phenotypes.txt')) as f:\n",
    "#     idp_ids = f.read().splitlines()\n",
    "idp_ids = ['FA_corpus_callosum', 'FA_uncinate_fasciculus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which data columns do we wish to use as covariates? \n",
    "cols_cov = ['age','sex']\n",
    "\n",
    "# which warping function to use? We can set this to None in order to fit a vanilla Gaussian noise model\n",
    "warp =  'WarpSinArcsinh'\n",
    "\n",
    "# limits for cubic B-spline basis \n",
    "xmin = -5 \n",
    "xmax = 110\n",
    "\n",
    "# Do we want to force the model to be refit every time? \n",
    "force_refit = True\n",
    "\n",
    "# Absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)\n",
    "outlier_thresh = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running IDP 0 FA_corpus_callosum :\n",
      "Estimating the normative model...\n",
      "Processing data in /home/preclineu/ramcir/Desktop/Diffusion/diffusion_nm/braincharts/models/test/FA_corpus_callosum/resp_tr.txt\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 191. MiB for an array with shape (4998, 1, 5008) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17607/3038453894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Estimating the normative model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         estimate(cov_file_tr, resp_file_tr, testresp=resp_file_te, \n\u001b[0m\u001b[1;32m     59\u001b[0m                  \u001b[0mtestcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov_file_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                  savemodel=True, warp=warp, warp_reparam=True)\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/pcntoolkit/normative.py\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(covfile, respfile, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mrun_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcvfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mXte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXte\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/pcntoolkit/dataio/fileio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mask, text, vol)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_nifti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/pcntoolkit/dataio/fileio.py\u001b[0m in \u001b[0;36mload_ascii\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;31m# based on pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mnshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 191. MiB for an array with shape (4998, 1, 5008) and data type float64"
     ]
    }
   ],
   "source": [
    "for idp_num, idp in enumerate(idp_ids): \n",
    "    print('Running IDP', idp_num, idp, ':')\n",
    "   \n",
    "    # set output dir \n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "    os.makedirs(os.path.join(idp_dir), exist_ok=True)\n",
    "    os.chdir(idp_dir)\n",
    "    \n",
    "    # extract the response variables for training and test set\n",
    "    y_tr = df_tr[idp].to_numpy() \n",
    "    y_te = df_te[idp].to_numpy()\n",
    "    \n",
    "    # remove gross outliers and implausible values\n",
    "#     yz_tr = (y_tr - np.mean(y_tr)) / np.std(y_tr)\n",
    "#     yz_te = (y_te - np.mean(y_te)) / np.std(y_te)\n",
    "#     nz_tr = np.bitwise_and(np.abs(yz_tr) < outlier_thresh, y_tr > 0)\n",
    "#     nz_te = np.bitwise_and(np.abs(yz_te) < outlier_thresh, y_te > 0)\n",
    "#     y_tr = y_tr[nz_tr]\n",
    "#     y_te = y_te[nz_te]\n",
    "    \n",
    "    # write out the response variables for training and test\n",
    "    resp_file_tr = os.path.join(idp_dir, 'resp_tr.txt')\n",
    "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt') \n",
    "    np.savetxt(resp_file_tr, y_tr)\n",
    "    np.savetxt(resp_file_te, y_te)\n",
    "        \n",
    "    # configure the design matrix\n",
    "    X_tr = create_design_matrix(df_tr[cols_cov], \n",
    "                                site_ids = df_tr['site'],\n",
    "                                basis = 'bspline', \n",
    "                                xmin = xmin, \n",
    "                                xmax = xmax)\n",
    "    X_te = create_design_matrix(df_te[cols_cov], \n",
    "                                site_ids = df_te['site'], \n",
    "                                all_sites=site_ids,\n",
    "                                basis = 'bspline', \n",
    "                                xmin = xmin, \n",
    "                                xmax = xmax)\n",
    "\n",
    "    # configure and save the covariates\n",
    "    cov_file_tr = os.path.join(idp_dir, 'cov_bspline_tr.txt')\n",
    "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
    "    np.savetxt(cov_file_tr, X_tr)\n",
    "    np.savetxt(cov_file_te, X_te)\n",
    "\n",
    "    if not force_refit and os.path.exists(os.path.join(idp_dir, 'Models', 'NM_0_0_estimate.pkl')):\n",
    "        print('Making predictions using a pre-existing model...')\n",
    "        suffix = 'predict'\n",
    "        \n",
    "        # Make prdictsion with test data\n",
    "        predict(cov_file_te, \n",
    "                alg='blr', \n",
    "                respfile=resp_file_te, \n",
    "                model_path=os.path.join(idp_dir,'Models'),\n",
    "                outputsuffix=suffix)\n",
    "    else:\n",
    "        print('Estimating the normative model...')\n",
    "        estimate(cov_file_tr, resp_file_tr, testresp=resp_file_te, \n",
    "                 testcov=cov_file_te, alg='blr', optimizer = 'l-bfgs-b', \n",
    "                 savemodel=True, warp=warp, warp_reparam=True)\n",
    "        suffix = 'estimate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise dataframe we will use to store quantitative metrics \n",
    "blr_metrics = pd.DataFrame(columns = ['eid', 'NLL', 'EV', 'MSLL', 'BIC','Skew','Kurtosis'])\n",
    "\n",
    "for idp_num, idp in enumerate(idp_ids): \n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "    \n",
    "    # load the predictions and true data. We use a custom function that ensures 2d arrays\n",
    "    # equivalent to: y = np.loadtxt(filename); y = y[:, np.newaxis]\n",
    "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_' + suffix + '.txt'))\n",
    "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_' + suffix + '.txt'))\n",
    "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
    "    \n",
    "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
    "        nm = pickle.load(handle) \n",
    "    \n",
    "    # compute error metrics\n",
    "    if warp is None:\n",
    "        metrics = evaluate(y_te, yhat_te)  \n",
    "        \n",
    "        # compute MSLL manually as a sanity check\n",
    "        y_tr_mean = np.array( [[np.mean(y_tr)]] )\n",
    "        y_tr_var = np.array( [[np.var(y_tr)]] )\n",
    "        MSLL = compute_MSLL(y_te, yhat_te, s2_te, y_tr_mean, y_tr_var)         \n",
    "    else:\n",
    "        warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] \n",
    "        W = nm.blr.warp\n",
    "        \n",
    "        # warp predictions\n",
    "        med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
    "        med_te = med_te[:, np.newaxis]\n",
    "       \n",
    "        # evaluation metrics\n",
    "        metrics = evaluate(y_te, med_te)\n",
    "        \n",
    "        # compute MSLL manually\n",
    "        y_te_w = W.f(y_te, warp_param)\n",
    "        y_tr_w = W.f(y_tr, warp_param)\n",
    "        y_tr_mean = np.array( [[np.mean(y_tr_w)]] )\n",
    "        y_tr_var = np.array( [[np.var(y_tr_w)]] )\n",
    "        MSLL = compute_MSLL(y_te_w, yhat_te, s2_te, y_tr_mean, y_tr_var)     \n",
    "    \n",
    "    Z = np.loadtxt(os.path.join(idp_dir, 'Z_' + suffix + '.txt'))\n",
    "    [skew, sdskew, kurtosis, sdkurtosis, semean, sesd] = calibration_descriptives(Z)\n",
    "    \n",
    "    BIC = len(nm.blr.hyp) * np.log(y_tr.shape[0]) + 2 * nm.neg_log_lik\n",
    "    \n",
    "    blr_metrics.loc[len(blr_metrics)] = [idp, nm.neg_log_lik, metrics['EXPV'][0], \n",
    "                                         MSLL[0], BIC, skew, kurtosis]\n",
    "    \n",
    "display(blr_metrics)\n",
    "\n",
    "blr_metrics.to_pickle(os.path.join(out_dir,'blr_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ids_tr = df_tr.site.unique()\n",
    "site_ids_te = df_te.site.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idp_num, idp in enumerate(idp_ids): \n",
    "    print('Running IDP', idp_num, idp, ':')\n",
    "    idp_dir = os.path.join(out_dir, idp)\n",
    "    os.chdir(idp_dir)\n",
    "    \n",
    "    # extract and save the response variables for the test set\n",
    "    y_te = df_te[idp].to_numpy()\n",
    "    \n",
    "    # save the variables\n",
    "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt') \n",
    "    np.savetxt(resp_file_te, y_te)\n",
    "        \n",
    "    # configure and save the design matrix\n",
    "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
    "    X_te = create_design_matrix(df_te[cols_cov], \n",
    "                                site_ids = df_te['site'],\n",
    "                                all_sites = site_ids_te,\n",
    "                                basis = 'bspline', \n",
    "                                xmin = xmin, \n",
    "                                xmax = xmax)\n",
    "    np.savetxt(cov_file_te, X_te)\n",
    "    \n",
    "    # check whether all sites in the test set are represented in the training set\n",
    "    if all(elem in site_ids_tr for elem in site_ids_te):\n",
    "        print('All sites are present in the training data')\n",
    "        \n",
    "        # just make predictions\n",
    "        yhat_te, s2_te, Z = predict(cov_file_te, \n",
    "                                    alg='blr', \n",
    "                                    respfile=resp_file_te, \n",
    "                                    model_path=os.path.join(idp_dir,'Models'))\n",
    "    else:\n",
    "        print('Some sites missing from the training data. Adapting model')\n",
    "        \n",
    "        # save the covariates for the adaptation data\n",
    "        X_ad = create_design_matrix(df_ad[cols_cov], \n",
    "                                    site_ids = df_ad['site'],\n",
    "                                    all_sites = site_ids_tr,\n",
    "                                    basis = 'bspline', \n",
    "                                    xmin = xmin, \n",
    "                                    xmax = xmax)\n",
    "        cov_file_ad = os.path.join(idp_dir, 'cov_bspline_ad.txt')          \n",
    "        np.savetxt(cov_file_ad, X_ad)\n",
    "        \n",
    "        # save the responses for the adaptation data\n",
    "        resp_file_ad = os.path.join(idp_dir, 'resp_ad.txt') \n",
    "        y_ad = df_ad[idp].to_numpy()\n",
    "        np.savetxt(resp_file_ad, y_ad)\n",
    "       \n",
    "        # save the site ids for the adaptation data\n",
    "        sitenum_file_ad = os.path.join(idp_dir, 'sitenum_ad.txt') \n",
    "        site_num_ad = df_ad['sitenum'].to_numpy(dtype=int)\n",
    "        np.savetxt(sitenum_file_ad, site_num_ad)\n",
    "        \n",
    "        # save the site ids for the test data \n",
    "        sitenum_file_te = os.path.join(idp_dir, 'sitenum_te.txt')\n",
    "        site_num_te = df_te['sitenum'].to_numpy(dtype=int)\n",
    "        np.savetxt(sitenum_file_te, site_num_te)\n",
    "         \n",
    "        yhat_te, s2_te, Z = predict(cov_file_te, \n",
    "                                    alg = 'blr', \n",
    "                                    respfile = resp_file_te, \n",
    "                                    model_path = os.path.join(idp_dir,'Models'),\n",
    "                                    adaptrespfile = resp_file_ad,\n",
    "                                    adaptcovfile = cov_file_ad,\n",
    "                                    adaptvargroupfile = sitenum_file_ad,\n",
    "                                    testvargroupfile = sitenum_file_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
